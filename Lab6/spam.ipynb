{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# CS660 Lab 6 (Spam Detector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "<https://en.wikipedia.org/wiki/Naive_Bayes_spam_filtering>\n",
    "\n",
    "The wikipedia article states that combining individual probabilities is done with the following formula\n",
    "\n",
    "$p = \\frac{p_1p_2...p_n}{p_1p_2...p_n + (1-p_1)(1-p_2)...(1-p_n)}$\n",
    "\n",
    "where $p_{n}$ is the probability  $P(S|W_{N})$  that it is a spam knowing it contains an Nth word (for example \"home\").\n",
    "\n",
    "However, $p_{n}$ should be  $P(W_{N}|S)$  that it is the probability that the Nth word appears if the message were spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# However the actual formula is \n",
    "\n",
    "# $P(S|a_1,a_2,...a_n) = \\frac{P(a_1,a_2,...a_n|S)P(S)}{P(a_1,a_2,...a_n|s)P(S) + P(a_1,a_2,...a_n|H)P(H)}$\n",
    "\n",
    "# The first item in the numerator and denominator correspond to $p_1, p_2, ... p_n$ in the wikipedia formula.\n",
    "\n",
    "# However, they then claim that $P(a_1,a_2,...a_n|H)P(H)$ (which actually corresponds $P(a_1,a_2,...a_n|H)P(H)$) is equal to $(1-p_1)(1-p_2)...(1-p_n)$.\n",
    "\n",
    "# This is not true because $P(A|S)P(S) \\neq 1 - P(A|H)P(H)$ even if $P(S) = 1 - P(H)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "### Notes\n",
    "$P(S) = \\frac{25}{100} = \\frac{1}{4}$\n",
    "\n",
    "$P(H) = \\frac{75}{100} = \\frac{3}{4}$\n",
    "\n",
    "$P(B | S) = \\frac{4}{5}, P(B | H) = \\frac{1}{15}$\n",
    "\n",
    "$P(C | S) = \\frac{3}{5}, P(C | H) = \\frac{2}{15}$\n",
    "\n",
    "$P(W | S) = \\frac{1}{5}, P(W | H) = \\frac{6}{15}$\n",
    "\n",
    "### Calculations\n",
    "$ P(S | B, C) = \\frac{P(B, C | S)P(S)}{P(B, C | S)P(S) + P(B, C | H)P(H)} = $\n",
    "$ \\frac{\\frac{4}{5}\\frac{3}{5}\\frac{1}{4}}{\\frac{4}{5}\\frac{3}{5}\\frac{1}{4} + \\frac{1}{15}\\frac{2}{15}\\frac{3}{4}} = $\n",
    "$ \\frac{\\frac{3}{25}}{\\frac{19}{150}} =  \\frac{18}{19} \\approx \\mathbf{0.947} $\n",
    "\n",
    "$ P(S | B) = \\frac{P(B | S)P(S)}{P(B | S)P(S) + P(B | H)P(H)} = $\n",
    "$ \\frac{\\frac{4}{5}\\frac{1}{4}}{\\frac{4}{5}\\frac{1}{4} + \\frac{1}{15}\\frac{3}{4}} = $\n",
    "$ \\frac{\\frac{1}{5}}{\\frac{1}{4}} = \\frac{4}{5} =  \\mathbf{0.8} $\n",
    "\n",
    "$ P(S | B, C, W) = \\frac{P(B, C, W | S)P(S)}{P(B, C, W | S)P(S) + P(B, C, W | H)P(H)} = $\n",
    "$ \\frac{\\frac{4}{5}\\frac{3}{5}\\frac{1}{5}\\frac{1}{4}}{\\frac{4}{5}\\frac{3}{5}\\frac{1}{5}\\frac{1}{4} + \\frac{1}{15}\\frac{2}{15}\\frac{6}{15}\\frac{3}{4}} = $\n",
    "$ \\frac{\\frac{3}{125}}{\\frac{2}{75}} = \\frac{9}{10} = \\mathbf{0.9} $\n",
    "\n",
    "### Comparison\n",
    "The addition of the word \"CHEAP\" to the email containing \"BUY\" *increases* the probability of \n",
    "the email being classified as spam spam. Both words have a high individual probability of being present in a spam email, \n",
    "therefore it is intuitive that the two of them together results in an increased spam classification probability.\n",
    "\n",
    "Conversely, the addition of the word \"WORK\" *decreases* the probability. The simple explanation is that because \n",
    "\"WORK\" appears in a higher number of *non-spam* emails, its presence *decreases* the probability of the email being spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "\n",
    "### Building a SPAM filter using Naive Bayes\n",
    "#### What is the problem? (Spam or not Spam: that is the question)\n",
    "The problem of determining whether an email is spam or *not* spam is a categorization/classification problem. \n",
    "Given an email (instance), and a set of categories: {\"spam\", \"not spam\"}, how can we determine what category the \n",
    "email belongs to?\n",
    "\n",
    "#### Bayesian Methods for Classification\n",
    "We can use **Bayes theorem** to build a **generative model** that approximates how data is produced. \n",
    "\n",
    "$$  P(C | X) = \\frac{P(X | C) P(C)}{P(X)} $$\n",
    "\n",
    "Because we only need the most probable category, we can simplify this to:\n",
    "\n",
    "$$ argmax_{c \\in C} \\text{ } P(X | c) P(c) $$\n",
    "\n",
    "If we assume all hypotheses are *a priori* equally likely, we must only consider \n",
    "the $P(X | c)$ term.\n",
    "\n",
    "#### Learning the Model\n",
    "Thanks to the Naive Bayes Conditional Independence Assumption, we can use the following \n",
    "simplification:\n",
    "\n",
    "$$ P(x_i, x_2, ..., x_n | c_j) = \\prod_{i} P(x_i | c_j) $$\n",
    "\n",
    "Given a set of training data (emails) of size $N$, where $count(X=x)$ is the number \n",
    "of emails for which $X=x$, e.g. spam=true. For each category $c$ and each value $x$ \n",
    "for a variable $X$\n",
    "\n",
    "$ P(c) = \\frac{count(C = c)}{|N|} $\n",
    "\n",
    "$ P(x | c) = \\frac{count(X = x, C = c)}{count(C = c)} $\n",
    "\n",
    "In other words, the probability of an email belonging to a certain category is equal \n",
    "to the total number of emails with that category, divided by the total number of \n",
    "training emails. The second equation can be intuited as the probability of a word \n",
    "showing up in an email, knowing that the email belongs to category $c$, is equal to \n",
    "the total count of that word among emails with that category, divided by the total \n",
    "number of training emails with that category.\n",
    "\n",
    "With training, we calculate the **prior probability** of each word belonging to a \n",
    "category (spam or not-spam). There exists a problem with this though.\n",
    "\n",
    "#### Overfitting\n",
    "What if the training data is not adequate; it may be missing a case. Zero probabilities \n",
    "overwhelm any other evidence. To fix this, we modify the $P(x | c)$ equation above \n",
    "to:\n",
    "\n",
    "$ P(x | c) = \\frac{count(X = x, C = c) + 1}{count(C = c) + |X|} $\n",
    "\n",
    "#### Underflow\n",
    "Multiplying lots of probabilities can result in floating-point underflow. For this, \n",
    "we can take advantage of a property of the $\\log$ function: $\\log(xy) = \\log(x) + \\log(y) $. \n",
    "So, we take the $\\log$ of each probability and *sum* them together instead of multiply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from operator import add\n",
    "from random import random\n",
    "from operator import add\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random: 1\n",
      "words: 5\n",
      "random: 1\n",
      "this: 2\n",
      "is: 2\n",
      "not: 1\n",
      "a: 1\n",
      "sentence: 1\n",
      "just: 1\n",
      "some: 1\n"
     ]
    }
   ],
   "source": [
    "lines = spark.read.text(\"./input/words.txt\").rdd.map(lambda r: r[0])\n",
    "counts = lines.flatMap(lambda x: x.split(\" \")).map(lambda x: (x, 1)).reduceByKey(add)\n",
    "\n",
    "output = counts.collect()\n",
    "\n",
    "for word, count in output:\n",
    "    print(\"%s: %i\" % (word, count))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Pi Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.142468\n"
     ]
    }
   ],
   "source": [
    "partitions = 2\n",
    "n = 1000000 * partitions\n",
    "\n",
    "def f(_):\n",
    "    x = random() * 2 - 1\n",
    "    y = random() * 2 - 1\n",
    "    return 1 if x ** 2 + y ** 2 <= 1 else 0\n",
    "\n",
    "count = spark.sparkContext.parallelize(range(1, n + 1), partitions).map(f).reduce(add)\n",
    "print(\"Pi is roughly %f\" % (4.0 * count / n))\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeContribs(urls, rank):\n",
    "    \"\"\"Calculates URL contributions to the rank of other URLs.\"\"\"\n",
    "    num_urls = len(urls)\n",
    "    for url in urls:\n",
    "        yield (url, rank / num_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseNeighbors(urls):\n",
    "    \"\"\"Parses a urls pair string into urls pair.\"\"\"\n",
    "    parts = re.split(r'\\s+', urls)\n",
    "    return parts[0], parts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank(urls, iterations):\n",
    "    lines = spark.read.text(urls).rdd.map(lambda r: r[0])\n",
    "    \n",
    "\n",
    "    # Loads all URLs from input file and initialize their neighbors.\n",
    "    links = lines.map(lambda urls: parseNeighbors(urls)).distinct().groupByKey().cache()\n",
    "    nodes =  links.count()\n",
    "    \n",
    "    # Loads all URLs with other URL(s) link to from input file and initialize ranks of them to one.\n",
    "    ranks = links.map(lambda url_neighbors: (url_neighbors[0], 1))\n",
    "    \n",
    "\n",
    "    for _ in range(iterations):\n",
    "        # Calculates URL contributions to the rank of other URLs.\n",
    "        contribs = links.join(ranks).flatMap(lambda url_urls_rank: computeContribs(url_urls_rank[1][0], url_urls_rank[1][1]))\n",
    "\n",
    "        # Re-calculates URL ranks based on neighbor contributions.\n",
    "        ranks = contribs.reduceByKey(add).mapValues(lambda rank: rank * 0.85 + 0.15)\n",
    "\n",
    "    # Collects all URL ranks and dump them to console.\n",
    "    for (link, rank) in ranks.collect():\n",
    "        print(\"%s has rank: %s.\" % (link, rank/nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 has rank: 0.1417773524409937.\n",
      "3 has rank: 0.28794878983600325.\n",
      "4 has rank: 0.20203676231820406.\n",
      "1 has rank: 0.36823709540479854.\n"
     ]
    }
   ],
   "source": [
    "pagerank(\"./input/pagerank_data.txt\", 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": true,
   "user_envs_cfg": true
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
