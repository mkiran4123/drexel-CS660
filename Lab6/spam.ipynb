{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# CS660 Lab 6 (Spam Detector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "<https://en.wikipedia.org/wiki/Naive_Bayes_spam_filtering>\n",
    "\n",
    "The wikipedia article states that combining individual probabilities is done with the following formula\n",
    "\n",
    "$p = \\frac{p_1p_2...p_n}{p_1p_2...p_n + (1-p_1)(1-p_2)...(1-p_n)}$\n",
    "\n",
    "However the actual formula is \n",
    "\n",
    "$P(S|a_1,a_2,...a_n) = \\frac{P(a_1,a_2,...a_n|S)P(S)}{P(a_1,a_2,...a_n|s)P(S) + P(a_1,a_2,...a_n|H)P(H)}$\n",
    "\n",
    "The first item in the numerator and denominator correspond to $p_1, p_2, ... p_n$ in the wikipedia formula.\n",
    "\n",
    "However, they then claim that $P(a_1,a_2,...a_n|H)P(H)$ (which actually corresponds $P(a_1,a_2,...a_n|H)P(H)$) is equal to $(1-p_1)(1-p_2)...(1-p_n)$.\n",
    "\n",
    "This is not true because $P(A|S)P(S) \\neq 1 - P(A|H)P(H)$ even if $P(S) = 1 - P(H)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "### Notes\n",
    "$P(S) = \\frac{25}{100} = \\frac{1}{4}$\n",
    "\n",
    "$P(H) = \\frac{75}{100} = \\frac{3}{4}$\n",
    "\n",
    "$P(B | S) = \\frac{4}{5}, P(B | H) = \\frac{1}{15}$\n",
    "\n",
    "$P(C | S) = \\frac{3}{5}, P(C | H) = \\frac{2}{15}$\n",
    "\n",
    "$P(W | S) = \\frac{1}{5}, P(W | H) = \\frac{6}{15}$\n",
    "\n",
    "### Calculations\n",
    "$ P(S | B, C) = \\frac{P(B, C | S)P(S)}{P(B, C | S)P(S) + P(B, C | H)P(H)} = $\n",
    "$ \\frac{\\frac{4}{5}\\frac{3}{5}\\frac{1}{4}}{\\frac{4}{5}\\frac{3}{5}\\frac{1}{4} + \\frac{1}{15}\\frac{2}{15}\\frac{3}{4}} = $\n",
    "$ \\frac{\\frac{3}{25}}{\\frac{19}{150}} =  \\frac{18}{19} \\approx \\mathbf{0.947} $\n",
    "\n",
    "$ P(S | B) = \\frac{P(B | S)P(S)}{P(B | S)P(S) + P(B | H)P(H)} = $\n",
    "$ \\frac{\\frac{4}{5}\\frac{1}{4}}{\\frac{4}{5}\\frac{1}{4} + \\frac{1}{15}\\frac{3}{4}} = $\n",
    "$ \\frac{\\frac{1}{5}}{\\frac{1}{4}} = \\frac{4}{5} =  \\mathbf{0.8} $\n",
    "\n",
    "$ P(S | B, C, W) = \\frac{P(B, C, W | S)P(S)}{P(B, C, W | S)P(S) + P(B, C, W | H)P(H)} = $\n",
    "$ \\frac{\\frac{4}{5}\\frac{3}{5}\\frac{1}{5}\\frac{1}{4}}{\\frac{4}{5}\\frac{3}{5}\\frac{1}{5}\\frac{1}{4} + \\frac{1}{15}\\frac{2}{15}\\frac{6}{15}\\frac{3}{4}} = $\n",
    "$ \\frac{\\frac{3}{125}}{\\frac{2}{75}} = \\frac{9}{10} = \\mathbf{0.9} $\n",
    "\n",
    "### Comparison\n",
    "The addition of the word \"CHEAP\" to the email containing \"BUY\" *increases* the probability of \n",
    "the email being classified as spam spam. Both words have a high individual probability of being present in a spam email, \n",
    "therefore it is intuitive that the two of them together results in an increased spam classification probability.\n",
    "\n",
    "Conversely, the addition of the word \"WORK\" *decreases* the probability. The simple explanation is that because \n",
    "\"WORK\" appears in a higher number of *non-spam* emails, its presence *decreases* the probability of the email being spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "\n",
    "### Building a SPAM filter using Naive Bayes\n",
    "#### What is the problem? (Spam or not Spam: that is the question)\n",
    "The problem of determining whether an email is spam or *not* spam is a categorization/classification problem. \n",
    "Given an email (instance), and a set of categories: {\"spam\", \"not spam\"}, how can we determine what category the \n",
    "email belongs to?\n",
    "\n",
    "#### Bayesian Methods for Classification\n",
    "We can use **Bayes theorem** to build a **generative model** that approximates how data is produced. \n",
    "\n",
    "$$  P(C | X) = \\frac{P(X | C) P(C)}{P(X)} $$\n",
    "\n",
    "Because we only need the most probable category, we can simplify this to:\n",
    "\n",
    "$$ argmax_{c \\in C} \\text{ } P(X | c) P(c) $$\n",
    "\n",
    "If we assume all hypotheses are *a priori* equally likely, we must only consider \n",
    "the $P(X | c)$ term.\n",
    "\n",
    "#### Learning the Model\n",
    "Thanks to the Naive Bayes Conditional Independence Assumption, we can use the following \n",
    "simplification:\n",
    "\n",
    "$$ P(x_i, x_2, ..., x_n | c_j) = \\prod_{i} P(x_i | c_j) $$\n",
    "\n",
    "Given a set of training data (emails) of size $N$, where $count(X=x)$ is the number \n",
    "of emails for which $X=x$, e.g. spam=true. For each category $c$ and each value $x$ \n",
    "for a variable $X$\n",
    "\n",
    "$ P(c) = \\frac{count(C = c)}{|N|} $\n",
    "\n",
    "$ P(x | c) = \\frac{count(X = x, C = c)}{count(C = c)} $\n",
    "\n",
    "In other words, the probability of an email belonging to a certain category is equal \n",
    "to the total number of emails with that category, divided by the total number of \n",
    "training emails. The second equation can be intuited as the probability of a word \n",
    "showing up in an email, knowing that the email belongs to category $c$, is equal to \n",
    "the total count of that word among emails with that category, divided by the total \n",
    "number of training emails with that category.\n",
    "\n",
    "With training, we calculate the **prior probability** of each word belonging to a \n",
    "category (spam or not-spam). There exists a problem with this though.\n",
    "\n",
    "#### Overfitting\n",
    "What if the training data is not adequate; it may be missing a case. Zero probabilities \n",
    "overwhelm any other evidence. To fix this, we modify the $P(x | c)$ equation above \n",
    "to:\n",
    "\n",
    "$ P(x | c) = \\frac{count(X = x, C = c) + 1}{count(C = c) + |X|} $\n",
    "\n",
    "#### Underflow\n",
    "Multiplying lots of probabilities can result in floating-point underflow. For this, \n",
    "we can take advantage of a property of the $\\log$ function: $\\log(xy) = \\log(x) + \\log(y) $. \n",
    "So, we take the $\\log$ of each probability and *sum* them together instead of multiply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from random import random\n",
    "from operator import add\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random: 1\n",
      "words: 5\n",
      "random: 1\n",
      "this: 2\n",
      "is: 2\n",
      "not: 1\n",
      "a: 1\n",
      "sentence: 1\n",
      "just: 1\n",
      "some: 1\n"
     ]
    }
   ],
   "source": [
    "lines = spark.read.text(\"./input/words.txt\").rdd.map(lambda r: r[0])\n",
    "counts = lines.flatMap(lambda x: x.split(\" \")).map(lambda x: (x, 1)).reduceByKey(add)\n",
    "\n",
    "output = counts.collect()\n",
    "\n",
    "for word, count in output:\n",
    "    print(\"%s: %i\" % (word, count))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Pi Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.142468\n"
     ]
    }
   ],
   "source": [
    "partitions = 2\n",
    "n = 1000000 * partitions\n",
    "\n",
    "def f(_):\n",
    "    x = random() * 2 - 1\n",
    "    y = random() * 2 - 1\n",
    "    return 1 if x ** 2 + y ** 2 <= 1 else 0\n",
    "\n",
    "count = spark.sparkContext.parallelize(range(1, n + 1), partitions).map(f).reduce(add)\n",
    "print(\"Pi is roughly %f\" % (4.0 * count / n))\n",
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
